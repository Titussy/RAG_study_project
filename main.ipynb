{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee818bc",
   "metadata": {},
   "source": [
    "# Setup for RAG\n",
    "\n",
    "* look at file `00-simple-local-rag.ipynb` for the detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496d9e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060', major=8, minor=9, total_memory=8187MB, multi_processor_count=24)\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "## checking local memoery\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384c6362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the embeddings from CSV file\n",
    "text_chunks_embeddings_df = pd.read_csv(\"text_chunks_embeddings_df.csv\")\n",
    "\n",
    "#Convert the embeddings from string representation to numpy arrays\n",
    "text_chunks_embeddings_df[\"embedding\"] = text_chunks_embeddings_df[\"embedding\"].apply(\n",
    "    lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \").astype(np.float64)\n",
    ")\n",
    "\n",
    "embeddings = torch.tensor(np.stack(text_chunks_embeddings_df[\"embedding\"].tolist(), \n",
    "                    axis=0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbac5637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG_study_project\\.venv-torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "try:\n",
    "    if embedding_model is None:\n",
    "        # Load the embedding model\n",
    "        embedding_model = SentenceTransformer(model_name_or_path= \"all-mpnet-base-v2\", device=device)\n",
    "except NameError:\n",
    "    embedding_model = SentenceTransformer(model_name_or_path= \"all-mpnet-base-v2\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fb729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pdf_img_show(num_page: int):\n",
    "\n",
    "    pdf_path = \"human-nutrution-text.pdf\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(num_page+41)\n",
    "\n",
    "    img = page.get_pixmap(dpi=300)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "    \n",
    "    plt.figure(figsize= (13,10))\n",
    "    plt.imshow(img_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64338dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrival_search(query : str,\n",
    "             top_k : int = 5,\n",
    "             embeddings : torch.tensor = embeddings,\n",
    "             embedding_model : SentenceTransformer = embedding_model,\n",
    "             \n",
    "             device: str = device):\n",
    "    \n",
    "    #step 1: Turn query to embedding\n",
    "    query_embedding = embedding_model.encode(query, device=device, convert_to_tensor=True).to(torch.float64)\n",
    "    \n",
    "    #step 2: Find top k <query_embedding, embeddings>\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    top_k = torch.topk(dot_scores, k=top_k+1)\n",
    "    \n",
    "    #step 3: text_chunk_df[top_k.indices]\n",
    "    indices = top_k.indices\n",
    "    scores = top_k.values\n",
    "    \n",
    "    return indices, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f59f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_top_results(query : str,        \n",
    "                      top_k : int = 5,\n",
    "                      embeddings : torch.tensor = embeddings,\n",
    "                      embedding_model : SentenceTransformer = embedding_model,\n",
    "                      text_chunk_df : pd.DataFrame = text_chunks_embeddings_df,\n",
    "                      show_result = True):\n",
    "    \n",
    "    ## find top k relevant text chunk\n",
    "    indices , scores = retrival_search(query, top_k, embeddings, embedding_model)\n",
    "    \n",
    "    top_k_relevant_text = [text_chunk_df[\"sentence_chunk\"].iloc[int(idx)] for idx in indices]\n",
    "    \n",
    "    if show_result:\n",
    "        print(f\"Query: {query}\\n\")\n",
    "        print(\"---\"*40)\n",
    "    \n",
    "    relevant_texts = list()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        text = text_chunk_df[\"sentence_chunk\"].iloc[int(idx)]\n",
    "        relevant_texts.append(text)\n",
    "        page_number = int(text_chunks_embeddings_df[\"page_number\"].iloc[int(idx)])\n",
    "        \n",
    "        if show_result:\n",
    "            pdf_img_show(page_number)\n",
    "            print(f\"Top {i+1} relevant text\")\n",
    "            print(textwrap.fill(text, 80)+\"\\n\")\n",
    "            print(f\"Source page: {page_number}\")\n",
    "            print(\"---\"*40)\n",
    "        \n",
    "    return relevant_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcec9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = gpu_memory_bytes / (1024 ** 3)\n",
    "print(f\"Total GPU memory: {gpu_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2b081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 7.99560546875 | Recommended model: Gemma 2B in 4-bit precision.\n",
      "use_quantization_config set to: True\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af9acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation set to: flash_attention_2\n",
      "Loading model google/gemma-2b-it ...\n",
      "Loading tokenizer for google/gemma-2b-it ...\n",
      "Loading model for google/gemma-2b-it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "# 1. Create a quantization \n",
    "#!pip install bitsandbytes accelerate\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
    "                                         bnb_4bit_compute_dtype = torch.float16)\n",
    "\n",
    "if(is_flash_attn_2_available) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"   ## scaled dot-product attention \n",
    "print(f\"Attention implementation set to: {attn_implementation}\")\n",
    "\n",
    "# 2. Loading LLM\n",
    "# \n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "model_id = model_id  \n",
    "print(  f\"Loading model {model_id} ...\")\n",
    "\n",
    "print(f\"Loading tokenizer for {model_id} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_id)\n",
    "\n",
    "\n",
    "print(f\"Loading model for {model_id} ...\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = model_id, \n",
    "                                          torch_dtype = torch.float16,\n",
    "                                          low_cpu_mem_usage=False,\n",
    "                                          #attn_implementation = attn_implementation,    ##very difficult to get flash attention work\n",
    "                                          quantization_config = quantization_config if use_quantization_config else None)\n",
    "\n",
    "if not use_quantization_config:\n",
    "    llm_model.to(\"cuda\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a955ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1515268096\n",
      "{'model_mem_bytes': 2039632384, 'model_mem_gb': 1.9}\n"
     ]
    }
   ],
   "source": [
    "## get the model parameters\n",
    "\n",
    "def get_model_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for  param in model.parameters()])\n",
    "\n",
    "print(f\"number of parameters: {get_model_params(llm_model)}\")\n",
    "\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    mem__params = sum([param.numel() * param.element_size() for param in model.parameters()])\n",
    "    mem_bufs = sum([buf.numel() * buf.element_size() for buf in model.buffers()])\n",
    "    total_mem_bytes = mem__params + mem_bufs      \n",
    "\n",
    "    total_mem_gb = total_mem_bytes / (1024 ** 3)\n",
    "\n",
    "    return {\"model_mem_bytes\": round(total_mem_bytes, 2),\n",
    "            \"model_mem_gb\": round(total_mem_gb, 2)}\n",
    "\n",
    "print(get_model_mem_size(llm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5443fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt: str, \n",
    "                    output_token_show: bool = False,\n",
    "                    output_text_show: bool = True):\n",
    "    #print(f\"Query_text: {prompt}\")\n",
    "\n",
    "    # Tokenize the input text and send it to the GPU\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  ## pt for pytorch\n",
    "\n",
    "    #print(\"\\nGenerating answer...\")\n",
    "    print(\"=====\"*20)\n",
    "    print(\"=====\"*20)\n",
    "\n",
    "    ## Generate outputs from Local LLM\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                max_new_tokens=256,\n",
    "                                temperature=0.7,\n",
    "                                do_sample=True\n",
    "                                )\n",
    "    \n",
    "    if output_token_show:\n",
    "        print(f\"Model output (tokens): \\n {outputs[0]}\\n\")\n",
    "        \n",
    "        print(\"=====\"*20)\n",
    "        print(\"=====\"*20)\n",
    "    \n",
    "    ### decode the output tokens to text\n",
    "    outputs_decoded = tokenizer.decode(outputs[0])\n",
    "    RAG_text = outputs_decoded.replace(prompt, '').strip()\n",
    "    print(f\"RAG output: \\n {textwrap.fill(RAG_text,80)}\\n\" )\n",
    "\n",
    "    return outputs_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4542e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## queries for testing\n",
    "\n",
    "queries = [\n",
    "    \"What are the six classes of nutrients required for the body to function, and what are their basic functions?\",\n",
    "    \"What is the primary role of carbohydrates in human nutrition, and how do they support specific cells like red blood cells and the brain?\",\n",
    "    \"How do simple and complex carbohydrates differ in terms of digestion and absorption in the body?\",\n",
    "    \"What are the five primary functions of carbohydrates in the human body according to the textbook?\",\n",
    "    \"How does dietary fiber contribute to health, and what examples of fiber-rich foods are mentioned in the context of the traditional Hawaiian diet?\",\n",
    "    \"What is the role of protein sparing in the context of carbohydrate consumption?\",\n",
    "    \"How do carbohydrates assist in lipid metabolism, and why is this important for energy use?\",\n",
    "    \"What are the health consequences of consuming too many or too few carbohydrates in the diet?\",\n",
    "    \"How does the liver redistribute glucose in the body, and what percentage of ingested glucose is typically redistributed?\",\n",
    "    \"Why is glucose the preferred energy source for certain cells, and under what conditions might the brain use alternative energy sources?\",\n",
    "    \"What are micronutrients, and how do they differ from macronutrients like carbohydrates in terms of function and energy provision?\",\n",
    "    \"What is the significance of water as a nutrient, and how much water does an average adult consume daily from food and drink?\",\n",
    "    \"How do vitamins and minerals function as micronutrients, and what is their role in enzymatic processes?\",\n",
    "    \"What are the benefits of a nutrient-dense diet, and how does it relate to maintaining a healthy weight?\",\n",
    "    \"How do carbohydrates contribute to building macromolecules, and what are some examples of these molecules?\",\n",
    "    \"What are the key nutritional components of the traditional Hawaiian diet, and what percentage of it was composed of carbohydrate-rich foods?\",\n",
    "    \"How does the body use glycogen, and where is it stored?\",\n",
    "    \"What are non-nutrients in foods, and how can they be beneficial or harmful to health?\",\n",
    "    \"How do carbohydrates support the nutritional needs of the brain and nervous system?\",\n",
    "    \"What factors affect an individual’s nutritional needs, and how can personal dietary choices impact health outcomes?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d331e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aims: \n",
    "\n",
    "create a prompt like:\n",
    "\n",
    "Based on the following contexts:\n",
    "- <context_item_1>\n",
    "- <context_item_2>\n",
    "- <context_item_3>\n",
    "\n",
    "Please answer the following query: {query}\n",
    "\n",
    "Anwser:\n",
    "\"\"\"\n",
    "def prompt_formatter(query:str ,\n",
    "                     contenxt_items: list[str],\n",
    "                     prompt_print = False) ->str:\n",
    "    \n",
    "    context = \"- \"+\"\\n- \".join(contenxt_items)\n",
    "\n",
    "    based_prompt = \"\"\"Based on the following contexts: \n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please answer the following query: {query}\n",
    "    Anwser:\"\"\"\n",
    "\n",
    "    based_prompt = based_prompt.format(context=context, query=query)\n",
    "    dialogue_template = [{\"role\": \"user\", \"content\": based_prompt}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(conversation = dialogue_template, \n",
    "                                            tokenize=False, \n",
    "                                            add_generation_prompt=True)\n",
    "\n",
    "\n",
    "    if prompt_print:\n",
    "        print(prompt)\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "\n",
    "## Another prompt formatting example to improve answer quality\n",
    "\n",
    "def prompt_formatter_2(query:str ,\n",
    "                     contenxt_items: list[str],\n",
    "                     prompt_print = False) ->str:\n",
    "    \n",
    "    context = \"- \"+\"\\n- \".join(contenxt_items)\n",
    "\n",
    "    based_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    prompt = based_prompt.format(context=context, query=query)\n",
    "    \n",
    "    if prompt_print:\n",
    "        print(prompt)\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb09078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def RAG_pipeline(query: str, \n",
    "                 top_k: int = 5,\n",
    "                 prompt_formatter = prompt_formatter_2):\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    print(\"Retrieving relevant contexts...\")\n",
    "    retrival_texts = print_top_results(query, show_result= False)\n",
    "\n",
    "    print(\"Formatting prompt with retrieved contexts...\")\n",
    "    augmented_prompt = prompt_formatter(query , retrival_texts)\n",
    "\n",
    "    print( \"Generating answer with augmented prompt...\")\n",
    "    generate_answer(augmented_prompt)\n",
    "    print(\"Done.\")\n",
    "# --- IGNORE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98573eb8",
   "metadata": {},
   "source": [
    "# RAG_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c24995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do carbohydrates support the nutritional needs of the brain and nervous system?\n",
      "Retrieving relevant contexts...\n",
      "Formatting prompt with retrieved contexts...\n",
      "Generating answer with augmented prompt...\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "RAG output: \n",
      " <bos> The passage does not provide any information about how carbohydrates\n",
      "support the nutritional needs of the brain and nervous system, so I cannot\n",
      "answer this query from the context.<eos>\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "## using pre-defined queries\n",
    "\n",
    "query = random.choice(queries) \n",
    "\n",
    "# define query yourself\n",
    "#query = \" What are the six classes of nutrients required for the body to function, and what are their basic functions?\"  \n",
    "\n",
    "RAG_pipeline(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975055b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
